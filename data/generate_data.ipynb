{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data\n",
    "\n",
    "Generates UMAP coordinates and model from HuggingFace datasets or a JSON file.\n",
    "\n",
    "In this notebook, we use the [Wikipedia sentences](https://huggingface.co/datasets/sentence-transformers/wikipedia-en-sentences) (HuggingFace) + [CHI 2024 paper titles](https://observablehq.com/@john-guerra/chi2024-papers) (JSON) datasets as examples. You may need to adjust the code to fit your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, PreTrainedTokenizer, PreTrainedModel\n",
    "import vec2text\n",
    "\n",
    "import umap\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from openai import OpenAI\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if models folder exists\n",
    "if not os.path.exists(\"../models\"):\n",
    "    os.makedirs(\"../models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the api keys from the secrets file\n",
    "try:\n",
    "    with open(\"../secrets.json\") as f:\n",
    "        secrets = json.load(f)\n",
    "    open_ai_key = secrets[\"openai\"]\n",
    "    print(\"API keys loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Secrets file not found. YOU NEED THEM TO RUN THIS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "llm = OpenAI(api_key=open_ai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random seed for reproducibility\n",
    "random_state = 42\n",
    "sample_size = 1000\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gtr_embeddings(text_list,\n",
    "                       encoder: PreTrainedModel,\n",
    "                       tokenizer: PreTrainedTokenizer) -> torch.Tensor:\n",
    "\n",
    "    inputs = tokenizer(text_list,\n",
    "                       return_tensors=\"pt\",\n",
    "                       max_length=128,\n",
    "                       truncation=True,\n",
    "                       padding=\"max_length\",).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output = encoder(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        hidden_state = model_output.last_hidden_state\n",
    "        embeddings = vec2text.models.model_utils.mean_pool(hidden_state, inputs['attention_mask'])\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "encoder = AutoModel.from_pretrained(\"sentence-transformers/gtr-t5-base\").encoder.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/gtr-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(sentences, dataset_name):\n",
    "    # batch embedding process\n",
    "    batch_size = 100\n",
    "    embeddings = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        embeddings.append(get_gtr_embeddings(sentences[i:i+batch_size], encoder, tokenizer))\n",
    "\n",
    "    # combine embeddings into one tensor\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "\n",
    "    # normalize embeddings\n",
    "    norms = torch.norm(embeddings, dim=1)\n",
    "    embeddings /= norms[:, None]\n",
    "\n",
    "    # save embeddings to file \n",
    "    torch.save(embeddings, f\"{dataset_name}/{dataset_name}_embeddings.pt\")\n",
    "\n",
    "    print(f\"Embeddings for {dataset_name} generated and saved to file.\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_umap(embeddings, sentences, dataset_name, n_neighbors=100, min_dist=0.1):\n",
    "    umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=2, metric='cosine', random_state=random_state)\n",
    "    reducer = umap_model.fit(embeddings.cpu())\n",
    "    umap_embeddings = reducer.transform(embeddings.cpu())\n",
    "\n",
    "    # save results to df\n",
    "    df_res = pd.DataFrame()\n",
    "    df_res['sentence'] = sentences\n",
    "    df_res['umap_x'] = umap_embeddings[:,0]\n",
    "    df_res['umap_y'] = umap_embeddings[:,1]\n",
    "\n",
    "    # save umap model\n",
    "    umap_path = f'../models/{dataset_name}_umap_reducer'\n",
    "    umap_pickle = open(umap_path, 'wb')\n",
    "    pickle.dump(reducer, umap_pickle)\n",
    "    umap_pickle.close()\n",
    "\n",
    "    print(f\"UMAP model for {dataset_name} saved to file.\")\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_clusters(cluster_samples):\n",
    "    cluster_examples = \"\" # input to LLM\n",
    "    for cluster, samples in cluster_samples.items():\n",
    "        cluster_examples += f'Cluster {cluster}\\n'\n",
    "        cluster_examples += '----------------\\n'\n",
    "        for sample in samples:\n",
    "            cluster_examples += sample + '\\n'\n",
    "        cluster_examples += '\\n'\n",
    "\n",
    "    system_prompt = f\"\"\"Given the example 10 sentences in each cluster, please assign a short phrase to describe each cluster. \n",
    "                    Format your answer as a dict where the key is the cluster number and the value is the phrase.\n",
    "                    Just output the dict, nothing else.\"\"\"\n",
    "    user_prompt = f\"{cluster_examples}\"\n",
    "    temperature = 0.1\n",
    "\n",
    "    # get completion from LLM (ask it to label the clusters)\n",
    "    completion = llm.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        temperature=temperature,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "    )\n",
    "    res = completion.choices[0].message.content\n",
    "\n",
    "    # read res as a dictionary\n",
    "    label_map = ast.literal_eval(res)\n",
    "    print(\"Cluster labels generated.\")\n",
    "    return label_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(df_res, embeddings, dataset_name, num_clusters=10):\n",
    "    # create copy of df_res\n",
    "    df_cluster = df_res.copy()\n",
    "\n",
    "    # create a kmeans model\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=random_state)\n",
    "\n",
    "    # fit the model\n",
    "    kmeans.fit(embeddings)\n",
    "\n",
    "    # get the cluster labels\n",
    "    cluster_labels = kmeans.labels_\n",
    "    df_cluster['cluster'] = cluster_labels\n",
    "\n",
    "    # randomly sample 10 sentences from each cluster\n",
    "    # and add them to a dictionary where the key is the cluster number\n",
    "    cluster_samples = {}\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_samples[cluster] = df_cluster[df_cluster['cluster'] == cluster].sample(10)['sentence'].tolist()\n",
    "\n",
    "    # get the labels for the clusters\n",
    "    label_map = label_clusters(cluster_samples)\n",
    "\n",
    "    # map df_cluster['cluster'] to the labels\n",
    "    df_cluster['cluster'] = df_cluster['cluster'].map(label_map)\n",
    "\n",
    "    # save data to json\n",
    "    df_cluster.to_json(f'{dataset_name}/{dataset_name}_data.json', orient='records')\n",
    "    \n",
    "    print(f\"Cluster info and umap coords for {dataset_name} saved to file.\")\n",
    "    return df_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(df_cluster):\n",
    "    # create interactive visualization of the embeddings\n",
    "    fig = px.scatter(df_cluster, x='umap_x', y='umap_y', hover_data=['sentence'], color='cluster')\n",
    "\n",
    "    width = 1000\n",
    "\n",
    "    # add a title and change the layout\n",
    "    fig.update_layout(title='UMAP of sentence embeddings with clusters',\n",
    "        autosize=False,\n",
    "        width=width,\n",
    "        height=400,\n",
    "    )\n",
    "\n",
    "    # show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Sentences (HuggingFace Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_load = \"sentence-transformers/wikipedia-en-sentences\" # TODO: REPLACE WITH ANY HUGGINGFACE DATASET\n",
    "data = load_dataset(dataset_to_load)\n",
    "\n",
    "dataset_name = \"wiki\" # TODO: REPLACE WITH YOUR DATASET NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if folder exists\n",
    "if not os.path.exists(dataset_name):\n",
    "    os.makedirs(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample n rows from the dataset\n",
    "n = 10000\n",
    "sample = data['train'].shuffle(seed=42).select(range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sample[:sample_size])\n",
    "# rename the column to 'Sentence'\n",
    "df = df.rename(columns={'sentence': 'Sentence'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['Sentence'].to_list() # get sentences from df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embeddings\n",
    "embeddings = generate_embeddings(sentences, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get umap coords\n",
    "df_res = run_umap(embeddings, sentences, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster the data\n",
    "df_cluster = get_clusters(df_res, embeddings, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the clusters\n",
    "visualize_clusters(df_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHI 2024 Papers (JSON Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json data\n",
    "file_name = \"chi2024data.json\" # TODO: REPLACE WITH YOUR FILE NAME\n",
    "dataset_name = \"chi\" # TODO: REPLACE WITH YOUR DATASET NAME\n",
    "\n",
    "with open(file_name, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if folder exists\n",
    "if not os.path.exists(dataset_name):\n",
    "    os.makedirs(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = data[\"contents\"]\n",
    "\n",
    "# get title from each paper\n",
    "titles = [paper[\"title\"].strip() for paper in papers]\n",
    "\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample 1000 transcripts from this dataset\n",
    "np.random.seed(random_state)\n",
    "sampled_indices = np.random.choice(len(titles), sample_size, replace=False)\n",
    "\n",
    "sentences = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embeddings\n",
    "embeddings = generate_embeddings(sentences, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get umap coords\n",
    "df_res = run_umap(embeddings, sentences, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster the data\n",
    "df_cluster = get_clusters(df_res, embeddings, dataset_name, num_clusters=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the clusters\n",
    "visualize_clusters(df_cluster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amplio-ZGv3CrDw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
